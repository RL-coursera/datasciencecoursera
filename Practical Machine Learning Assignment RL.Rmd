---
title: "Practical Machine Learning Assignment"
author: "R Levine"
date: "12/6/2019"
output: html_document
---


## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Approach

The outcome variable is the classe, a factor variable of 5 levels:

A - exactly according to the specification
B - throwing the elbows to the front
C - lifting the dumbbell only halfway
D - lowering the dumbbell only halfway
E - throwing the hips to the front

Classe A corresponds to the specified execution of the exercise the 10 people have been asked to perform, while the other classes correspond to common mistakes. All other variables in the data set will be used as predictors, after cleaning.

Model evaluation will be based on maximizing the accuracy and minimizing the out of sample error. We  build two different models, using random forest and decision trees. The model with the highest accuracy will be choosen and validated on the original dataset.

We will subset the training set into two subsamples:

1. training data set, as 70% of the original data set
2. validation data set, as 30% of the original data set.  We will fit the model using the training data set and test it on the validation data set. Once the best model is chosen, it will be validated on the original data set

## Training and Test Data

The training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har

## Load Packages

Load packages needed for analysis. 

```{r loadpackages, echo=FALSE}
library(caret)
library(RCurl)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(ggplot2)
library(devtools)
devtools::install_github("taiyun/corrplot", build_vignettes = TRUE)
library(corrplot)
install.packages("randomForest",repos = "http://cran.us.r-project.org")
library(randomForest)
install.packages("rattle",repos = "http://cran.us.r-project.org")
library(rattle)
set.seed(54321)
```

## Load and Partition Data

```{r loaddata, echo=FALSE}
trainingLink <- getURL("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
pml_CSV  <- read.csv(text = trainingLink, header=TRUE, sep=",", na.strings=c("NA",""))
pml_CSV <- pml_CSV[,-1] # Remove the first column that represents a ID Row
# create training and validation datasets with 70% and 30% of the original dataset, respectively
inTrain = createDataPartition(pml_CSV$classe, p=0.70, list=FALSE)
training = pml_CSV[inTrain,]
validating = pml_CSV[-inTrain,]
# number of rows and columns in the training set
dim(training)
# number of rows and columns in the validating set
dim(validating)
```

Since we choose a random forest model and we have a data set with too many columns, first we check if we have issues with columns without data. We remove columns that have less than 60% of data entered.

## EDA and Data Cleaning

```{r EDA, echo=TRUE, results='asis'}
# Number of cols with less than 60% of data
sum((colSums(!is.na(training[,-ncol(training)])) < 0.6*nrow(training)))
# apply our definition of remove columns that most doesn't have data, before applying to the model.
Keep <- c((colSums(!is.na(training[,-ncol(training)])) >= 0.6*nrow(training)))
training   <-  training[,Keep]
validating <- validating[,Keep]
# remove cases that have many missing/NA values
NaValues <- sapply(training, function(x) mean(is.na(x))) > 0.9
training <- training[, NaValues == "FALSE"]
validating <- validating[, NaValues == "FALSE"]
# remove id and time variables
training <- training[,-c(1:5)]
validating <- validating[,-c(1:5)]
# number of rows and columns of data in the final training set
dim(training)
# number of rows and columns of data in the final validating set
dim(validating)
```

## Model with Random Forest

```{r rfm, echo=TRUE}
model <- randomForest(classe~.,data=training)
print(model)
## calculate predicted values 
pred_rf <- predict(model, validating)
## calculate confusion matrix
cm_rf <- confusionMatrix(pred_rf, validating$classe)
print(cm_rf)
plot(model)
```

## Model with Decision Tree

```{r dtm, echo=TRUE}
model_dt <- rpart(classe ~., data = training, method = "class")

##calculate predicted values
pred_dt <- predict(model_dt, validating, type = "class")

##calculate the confusion matrix
cm_dt <- confusionMatrix(pred_dt, validating$classe)
print(cm_dt)
```

The accuracy of this model is 80% which is not bad but much worse than that of the random forest model at 99.8%, so the latter will be chosen as the final model. 

## Final Prediction and Conclusion

```{r pred, echo=TRUE}
finalPred <- predict(model, training, type = "class")
head(finalPred,20)
```

The predicted classes using the random forest model are above. 
